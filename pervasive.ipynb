{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code.\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class bcolors:\n",
    "    # https://stackoverflow.com/questions/287871/how-to-print-colored-text-in-terminal-in-python\n",
    "    DEFAULT = '\\x1b[0m'\n",
    "    RED = '\\x1b[31m'\n",
    "    GREEN = '\\x1b[32m'\n",
    "    YELLOW = '\\x1b[33m'\n",
    "    CYAN = '\\x1b[36m'\n",
    "\n",
    "    DEBUG = CYAN\n",
    "    INFO = GREEN\n",
    "    WARNING = YELLOW\n",
    "    ERROR = RED\n",
    "    CRITICAL = RED\n",
    "\n",
    "\n",
    "def getLogger(logname):\n",
    "    return PervasiveLogger(logname)\n",
    "\n",
    "\n",
    "class PervasiveLogger(object):\n",
    "    \"\"\"\n",
    "    The usual logging library is not able to print to the terminal, because\n",
    "    it is run in a forked process. This logger fixes that.\n",
    "    \n",
    "    TODO: Derive it from a class in the Python logger library.\n",
    "    \"\"\"\n",
    "    def __init__(self, logname):\n",
    "        self.logname = logname\n",
    "        self.logfile = None\n",
    "        self.rank = os.getenv('RANK', None)\n",
    "\n",
    "    def write(self, msg, level, color=None, *args):\n",
    "        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        msg = msg % args\n",
    "        prank = f'P{self.rank}' if self.rank else ''\n",
    "        if color:\n",
    "            full_msg = f'{ts} {prank} {color}{msg}{bcolors.DEFAULT}'\n",
    "        else:\n",
    "            full_msg = f'{ts} {prank} {msg}'\n",
    "        print(full_msg)\n",
    "        if self.logfile:\n",
    "            with open(self.logfile, 'a') as f:\n",
    "                f.write(full_msg)\n",
    "\n",
    "    def set_logfile(self, logfile):\n",
    "        self.logfile = logfile\n",
    "\n",
    "    def critical(self, msg, *args):\n",
    "        self.write(msg, 'CRIT', bcolors.CRITICAL, *args)\n",
    "\n",
    "    def warning(self, msg, *args):\n",
    "        self.write(msg, 'WARN', bcolors.WARNING, *args)\n",
    "\n",
    "    def info(self, msg, *args):\n",
    "        self.write(msg, 'INFO', bcolors.INFO, *args)\n",
    "\n",
    "    def error(self, msg, *args):\n",
    "        self.write(msg, 'ERROR', bcolors.ERROR, *args)\n",
    "\n",
    "    def debug(self, msg, *args):\n",
    "        self.write(msg, 'DEBUG', bcolors.DEBUG, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code.\n",
    "\n",
    "import fire\n",
    "import yaml\n",
    "\n",
    "\n",
    "class PervasiveApp(object):\n",
    "    \n",
    "    def train(config, gpu_ids=None, verbose=True):\n",
    "        with open(config, 'r') as f:\n",
    "            params = yaml.load(f)\n",
    "\n",
    "        default_config = params.get('default_config', None)\n",
    "        if default_config:\n",
    "            with open(default_config, 'r') as f:\n",
    "                params.update(yaml.load(f))\n",
    "\n",
    "        if not gpu_ids:\n",
    "            if 'gpu_ids' not in params:\n",
    "                raise Exception('Expected parameter \"gpu_ids\" not supplied.')\n",
    "            gpu_ids = params['gpu_ids']\n",
    "            if not isinstance(gpu_ids, list):\n",
    "                gpu_ids = [gpu_ids]\n",
    "        \n",
    "        project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        if 'model_name' not in params:\n",
    "            raise Exception('Expected parameter \"model_name\" not supplied.')\n",
    "        events_path = os.path.join(project_dir, 'events', params['model_name'])\n",
    "        save_path = os.path.join(project_dir, 'save', params['model_name'])\n",
    "\n",
    "        logger = PervasiveLogger(jobname)\n",
    "        logger.info(f'Distributing to GPUs: {\", \".join(gpu_ids)}')\n",
    "        \n",
    "        os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "        os.environ['MASTER_PORT'] = '3892'\n",
    "        torch.multiprocessing.spawn(\n",
    "            train_worker, args=(gpu_ids, params), nprocs=len(gpu_ids), join=True)\n",
    "        logger.info(f'All {len(gpu_ids)} training processes joined. Shutting down.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(PervasiveApp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function from https://github.com/elbayadm/attn2d/blob/master/nmt/models/aggregator.py\n",
    "#\n",
    "\n",
    "def truncated_max(tensor, src_lengths):\n",
    "    \"\"\"\n",
    "    Max-pooling up to effective length\n",
    "    \n",
    "    input size: N, d, Tt, Ts\n",
    "    src_lengths : N,\n",
    "    \"\"\"\n",
    "    Pool = []\n",
    "    Attention = []\n",
    "    for n in range(tensor.size(0)):\n",
    "        X = tensor[n]\n",
    "        xpool, attn = X[:, :, :src_lengths[n]].max(dim=2)\n",
    "        Pool.append(xpool.unsqueeze(0))\n",
    "    result = torch.cat(Pool, dim=0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def truncated_mean(tensor, src_lengths):\n",
    "    \"\"\"\n",
    "    Average-pooling up to effective length\n",
    "\n",
    "    input size: N, d, Tt, Ts\n",
    "    src_lengths : N,\n",
    "    \"\"\"\n",
    "    Pool = []\n",
    "    Attention = []\n",
    "    for n in range(tensor.size(0)):                                                            \n",
    "        X = tensor[n]\n",
    "        xpool = X[:, :, :src_lengths[n]].mean(dim=2)                                           \n",
    "        xpool *=  math.sqrt(src_lengths[n])                                                    \n",
    "        Pool.append(xpool.unsqueeze(0))                                                        \n",
    "    result = torch.cat(Pool, dim=0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def average_code(tensor, src_lengths=None):\n",
    "    return tensor.mean(dim=3)\n",
    "\n",
    "\n",
    "def max_code(tensor, src_lengths=None):\n",
    "    return tensor.max(dim=3)[0]\n",
    "\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements max pool layer, etc.\n",
    "    \n",
    "    Adapted from https://github.com/elbayadm/attn2d/blob/master/nmt/models/aggregator.py\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channls, mode, output_channels=None, params={}):\n",
    "        nn.Module.__init__(self)\n",
    "        mode = params.get(\"mode\", \"max\")\n",
    "        self.output_channels = input_channls\n",
    "        if mode == 'mean':\n",
    "            self.project = average_code\n",
    "        elif mode == 'max':\n",
    "            self.project = max_code\n",
    "        elif mode == 'truncated-max':\n",
    "            self.project = truncated_max\n",
    "        elif mode == 'truncated-mean':\n",
    "            self.project = truncated_mean\n",
    "        else:\n",
    "            raise ValueError('Unknown mode %s' % mode)\n",
    "        # Map the final output to the requested dimension\n",
    "        # for when tying the embeddings with the final projection layer\n",
    "        print(self.output_channels, end='')\n",
    "        lin = nn.Linear(self.output_channels, force_output_channels)\n",
    "        print(\">\", force_output_channels)\n",
    "        self.output_channels = force_output_channels\n",
    "\n",
    "    def forward(self, tensor, src_lengths):\n",
    "        proj = self.project(tensor, src_lengths)\n",
    "        proj = proj.permute(0, 2, 1)\n",
    "        return self.lin(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Masked (autoregressive) conv2d.\n",
    "    \n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/models/conv2d.py\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=3, dilation=1,\n",
    "                 groups=1, bias=False):\n",
    "        pad = (dilation * (kernel_size - 1)) // 2\n",
    "        super(MaskedConv2d, self).__init__(in_channels, out_channels,\n",
    "                                           kernel_size,\n",
    "                                           padding=pad,\n",
    "                                           groups=groups,\n",
    "                                           dilation=dilation,\n",
    "                                           bias=bias)\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        if kH > 1:\n",
    "            self.mask[:, :, kH // 2 + 1:, :] = 0\n",
    "        self.incremental_state = torch.zeros(1, 1, 1, 1)\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "\n",
    "    def update(self, x):\n",
    "        k = self.weight.size(2) // 2 + 1\n",
    "        buffer = self.incremental_state\n",
    "        if buffer.size(2) < k:\n",
    "            output = self.forward(x)\n",
    "            self.incremental_state = x.clone()\n",
    "        else:\n",
    "            # Shift the buffer and add the recent input.\n",
    "            buffer[:, :, :-1, :] = buffer[:, :, 1:, :].clone()\n",
    "            buffer[:, :, -1:, :] = x[:, :, -1:, :]\n",
    "            output = self.forward(buffer)\n",
    "            self.incremental_state = buffer.clone()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_function_factory(norm, relu, conv):\n",
    "    \"\"\"\n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/models/efficient_densenet.py\n",
    "    \"\"\"\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "def init_weights(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize weights of all submodules.\n",
    "    Not object-oriented, but prevents repetitive code.\n",
    "\n",
    "    Cf. https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py#L46-L59\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer of a DenseNet.\n",
    "    \n",
    "    Adapted from https://github.com/elbayadm/attn2d/blob/master/nmt/models/efficient_densenet.py\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 growth_rate,\n",
    "                 kernel_size=3,\n",
    "                 bn_size=4,\n",
    "                 dropout=0,\n",
    "                 efficient=False):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.efficient = efficient\n",
    "        \n",
    "        self.add_module('norm1', nn.BatchNorm2d(input_size)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(\n",
    "            input_size,\n",
    "            bn_size * growth_rate,\n",
    "            kernel_size=1,\n",
    "            bias=False))\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', MaskedConv2d(\n",
    "            bn_size * growth_rate,\n",
    "            growth_rate,\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias))\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.efficient and any(prev_feature.requires_grad\n",
    "                                  for prev_feature in prev_features):\n",
    "            # Wins decreased memory at cost of extra computation.\n",
    "            # Does not compute intermediate values, but recomputes them in backward pass.\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.dropout > 0:\n",
    "            new_features = F.dropout(new_features, p=self.dropout,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "    def reset_buffers(self):\n",
    "        self.conv2.incremental_state = torch.zeros(1, 1, 1, 1)\n",
    "\n",
    "    def update(self, x):\n",
    "        maxh = self.kernel_size // 2 + 1\n",
    "        if x.size(2) > maxh:\n",
    "            x = x[:, :, -maxh:, :].contiguous()\n",
    "        res = x\n",
    "        x = self.conv1(self.relu1(self.norm1(x)))\n",
    "        x = self.conv2.update(self.relu2(self.norm2(x)))\n",
    "        return torch.cat([res, x], 1)\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Block of layers in a DenseNet.\n",
    "    \n",
    "    Adapted from https://github.com/elbayadm/attn2d/blob/master/nmt/models/efficient_densenet.py\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, input_size, bn_size, growth_rate,\n",
    "                 dropout, efficient=False):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        print('Dense channels:', input_size, end='')\n",
    "        for i in range(num_layers):\n",
    "            print(\">\", input_size + (i+1) * growth_rate, end='')\n",
    "            layer = DenseLayer(\n",
    "                input_size + i * growth_rate,\n",
    "                growth_rate,\n",
    "                kernels[i],\n",
    "                bn_size,\n",
    "                dropout,\n",
    "                efficient=efficient\n",
    "            )\n",
    "            self.add_module(f'denselayer{i + 1}', layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "    def update(self, x):\n",
    "        for layer in list(self.children()):\n",
    "            x = layer.update(x)\n",
    "        return x\n",
    "\n",
    "    def reset_buffers(self):\n",
    "        for layer in list(self.children()):\n",
    "            layer.reset_buffers()\n",
    "\n",
    "\n",
    "class Transition(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Transiton layer between dense blocks to reduce number of channels.\n",
    "    \n",
    "    BN > ReLU > Conv(k=1)\n",
    "    \n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/models/efficient_densenet.py\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(input_size))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        conv = nn.Conv2d(\n",
    "                input_size,\n",
    "                output_size,\n",
    "                kernel_size=1,\n",
    "                bias=False)\n",
    "        self.add_module('conv', conv)\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return super(Transition, self).forward(x)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\" \n",
    "    Much more memory efficient, but slower.\n",
    "    \n",
    "    Adapted from https://github.com/elbayadm/attn2d/blob/master/nmt/models/efficient_densenet.py\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, block_layers, bn_size=4, kernel_size=3,\n",
    "                 dropout=0.2, growth_rate=32, divide_channels=2,\n",
    "                 efficient=False):\n",
    "        super(EfficientDenseNet, self).__init__()\n",
    "\n",
    "        self.efficient = efficient\n",
    "\n",
    "        self.model = nn.Sequential()\n",
    "        num_features = input_size\n",
    "        if divide_channels > 1:\n",
    "            trans = nn.Conv2d(num_features, num_features // divide_channels, 1)\n",
    "            torch.nn.init.xavier_normal_(trans.weight)\n",
    "            self.model.add_module('initial_transition', trans)\n",
    "            num_features = num_features // divide_channels\n",
    "\n",
    "        for i, num_layers in enumerate(block_layers):\n",
    "            block = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                input_size=num_features,\n",
    "                kernels=kernels,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                dropout=dropout,\n",
    "                efficient=efficient\n",
    "            )\n",
    "            self.model.add_module(f'denseblock{i + 1}', block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            trans = Transition(\n",
    "                input_size=num_features,\n",
    "                output_size=num_features // 2)\n",
    "            self.model.add_module(f'transition{i + 1}', trans)\n",
    "            num_features = num_features // 2\n",
    "            print(\"> (trans) \", num_features)\n",
    "\n",
    "        self.output_channels = num_features\n",
    "        self.model.add_module('norm_final', nn.BatchNorm2d(num_features))\n",
    "        self.model.add_module('relu_last', nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.model(x.contiguous())\n",
    "\n",
    "    def update(self, x):\n",
    "        x = x.contiguous()\n",
    "        for layer in list(self.model.children()):\n",
    "            if isinstance(layer, DenseBlock):\n",
    "                x = layer.update(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def reset_buffers(self):\n",
    "        for layer in list(self.model.children()):\n",
    "            if isinstance(layer, DenseBlock):\n",
    "                layer.reset_buffers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Pervasive(nn.Module):\n",
    "    \"\"\"\n",
    "    Pervasive Attention Network.\n",
    "    \n",
    "    Based on https://github.com/elbayadm/attn2d/blob/master/nmt/models/pervasive.py\n",
    "    \"\"\"\n",
    "    \n",
    "    PAD = 0\n",
    "    \n",
    "    def __init__(self, name, net_type, Ts, Tt, special_tokens,\n",
    "                 enc_input_dim=128, enc_dropout=0.2,\n",
    "                 dec_input_dim=128, dec_dropout=0.2,\n",
    "                 prediction_dropout=0.2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = trg_vocab_size\n",
    "        self.enc_dropout = enc_dropout\n",
    "        self.dec_dropout = dec_dropout\n",
    "        self.padding_idx = special_tokens['PAD']\n",
    "        self.bos_token = special_tokens['BOS']\n",
    "        self.eos_token = special_tokens['EOS']\n",
    "        self.kernel_size = kernel_size\n",
    "        self.src_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            enc_input_dim\n",
    "            padding_idx,\n",
    "            scale_grad_by_freq=False\n",
    "        )\n",
    "        self.tgt_embedding = nn.Embedding(\n",
    "            self.tgt_vocab_size,\n",
    "            dec_input_dim,\n",
    "            dec_input_dropout,\n",
    "            padding_idx=self.padding_idx\n",
    "        )\n",
    "\n",
    "        self.input_channels = \\\n",
    "            self.src_embedding.dimension + self.tgt_embedding.dimension\n",
    "\n",
    "        self.logger.info('Model input channels: %d', self.input_channels)\n",
    "\n",
    "        if divison_factor > 1:\n",
    "            self.logger.info('Reducing the input channels by a factor of {division_factor}.')\n",
    "\n",
    "        if net_type == \"densenet\":\n",
    "            self.net = DenseNet(self.input_channels)\n",
    "        elif net_type == \"efficient-densenet\":\n",
    "            self.net = DenseNet(self.input_channels, efficient=True)\n",
    "        elif net_type == \"log-densenet\":\n",
    "            raise NotImplementedError('Log DenseNet not implemented.')\n",
    "        else:\n",
    "            raise ValueError(f'Unknown network type {net_type}.')\n",
    "\n",
    "        self.logger.warning('Tying the decoder weights.')\n",
    "        self.prediction.weight = self.tgt_embedding.label_embedding.weight\n",
    "        \n",
    "        self.aggregator = Aggregator(self.net.output_channels,\n",
    "                                     dec_input_dim,\n",
    "                                     params['aggregator'])\n",
    "        self.final_output_channels = self.aggregator.output_channels\n",
    "\n",
    "        self.prediction_dropout = nn.Dropout(prediction_dropout)\n",
    "        self.logger.info('Output channels: %d', self.final_output_channels)\n",
    "        self.prediction = nn.Linear(self.final_output_channels,\n",
    "                                    self.trg_vocab_size)     \n",
    "\n",
    "    def init_weights(self):\n",
    "        # Tensorflow default embedding initialization (except they resample instead of clipping).\n",
    "        src_std = 1 / math.sqrt(self.src_vocab_size)\n",
    "        torch.nn.init.normal_(self.src_embedding, 0, src_std)\n",
    "        torch.clamp_(self.src_embedding, min=-2*src_std, max=2*src_std)\n",
    "        tgt_std = 1 / math.sqrt(self.tgt_vocab_size)\n",
    "        torch.nn.init.normal_(self.tgt_embedding, 0, tgt_std)\n",
    "        torch.clamp_(self.tgt_embedding, min=-2*tgt_std, max=2*tgt_std)\n",
    "        init_weights(self.net)\n",
    "        init_weights(self.aggregator)\n",
    "        init_weights(self.prediction)\n",
    "\n",
    "    def forward(self, src_data, tgt_data):\n",
    "        src_emb = F.dropout(self.src_embedding(src_data), p=self.enc_dropout)\n",
    "        tgt_emb = F.dropout(self.tgt_embedding(tgt_data), p=self.dec_dropout)\n",
    "        Ts = src_emb.size(1)  # Source sequence length.\n",
    "        Tt = tgt_emb.size(1)  # Target sequence length.\n",
    "                    emb = F.dropout(emb,\n",
    "                            p=self.dropout,\n",
    "                            training=self.training)\n",
    "        src_emb = src_emb.unsqueeze(1).repeat(1, Tt, 1, 1)\n",
    "        tgt_emb = tgt_emb.unsqueeze(2).repeat(1, 1, Ts, 1)\n",
    "        \n",
    "        X = torch.cat((src_emb, trg_emb), dim=3)\n",
    "        X = X.permute(0, 3, 1, 2)\n",
    "        X = self.net(X)\n",
    "        X = self.aggregator(X, src_data['lengths'])\n",
    "        \n",
    "        logits = F.log_softmax(\n",
    "            self.prediction(self.prediction_dropout(X)), dim=2)\n",
    "        return logits\n",
    "\n",
    "    def update(self, X, src_lengths=None):\n",
    "        X = X.permute(0, 3, 1, 2)\n",
    "        X = self.net.update(X)\n",
    "        X = self.aggregator(X, src_lengths)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# From https://github.com/elbayadm/attn2d/blob/master/nmt/utils/utils.py\n",
    "#\n",
    "\n",
    "def pload(path):\n",
    "    \"\"\"\n",
    "    Pickle load\n",
    "    \"\"\"\n",
    "    return pickle.load(open(path, 'rb'),\n",
    "                       encoding='iso-8859-1')\n",
    "\n",
    "\n",
    "def pdump(obj, path):\n",
    "    \"\"\"\n",
    "    Picke dump\n",
    "    \"\"\"\n",
    "    pickle.dump(obj, open(path, 'wb'),\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_val_loss(job_name, trainer, src_loader, trg_loader, eval_kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate model.\n",
    "\n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/models/evaluate.py\n",
    "    \"\"\"\n",
    "    ground_truths = []\n",
    "    batch_size = eval_kwargs.get('batch_size', 1)\n",
    "    max_samples = eval_kwargs.get('max_samples', -1)\n",
    "    split = eval_kwargs.get('split', 'val')\n",
    "    verbose = eval_kwargs.get('verbose', 0)\n",
    "    eval_kwargs['BOS'] = trg_loader.bos\n",
    "    eval_kwargs['EOS'] = trg_loader.eos\n",
    "    eval_kwargs['PAD'] = trg_loader.pad\n",
    "    eval_kwargs['UNK'] = trg_loader.unk\n",
    "    logger = logging.getLogger(job_name)\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model = trainer.model\n",
    "    crit = trainer.criterion\n",
    "    model.eval()\n",
    "    src_loader.reset_iterator(split)\n",
    "    trg_loader.reset_iterator(split)\n",
    "    n = 0\n",
    "    loss_sum = 0\n",
    "    ml_loss_sum = 0\n",
    "    loss_evals = 0\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        # get batch\n",
    "        data_src, order = src_loader.get_src_batch(split, batch_size)\n",
    "        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n",
    "        n += batch_size\n",
    "        if model.version == 'seq2seq':\n",
    "            source = model.encoder(data_src)\n",
    "            source = model.map(source)\n",
    "            if trainer.criterion.version == \"seq\":\n",
    "                losses, stats = crit(model, source, data_trg)\n",
    "            else:  # ML & Token-level\n",
    "                # init and forward decoder combined\n",
    "                decoder_logit = model.decoder(source, data_trg)\n",
    "                losses, stats = crit(decoder_logit, data_trg['out_labels'])\n",
    "        else:\n",
    "            losses, stats = crit(model(data_src, data_trg), data_trg['out_labels'])\n",
    "\n",
    "        loss_sum += losses['final'].data.item()\n",
    "        ml_loss_sum += losses['ml'].data.item()\n",
    "        loss_evals = loss_evals + 1\n",
    "        if max_samples == -1:\n",
    "            ix1 = data_src['bounds']['it_max']\n",
    "        else:\n",
    "            ix1 = max_samples\n",
    "        if data_src['bounds']['wrapped']:\n",
    "            break\n",
    "        if n >= ix1:\n",
    "            break\n",
    "    logger.warn('Evaluated %d samples in %.2f s', n, time.time()-start)\n",
    "    return ml_loss_sum / loss_evals, loss_sum / loss_evals\n",
    "\n",
    "\n",
    "def evaluate_model(job_name, trainer, src_loader, trg_loader, eval_kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate model.\n",
    "\n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/models/evaluate.py\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    batch_size = eval_kwargs.get('batch_size', 1)\n",
    "    max_samples = eval_kwargs.get('max_samples', -1)\n",
    "    split = eval_kwargs.get('split', 'val')\n",
    "    verbose = eval_kwargs.get('verbose', 0)\n",
    "    eval_kwargs['BOS'] = trg_loader.bos\n",
    "    eval_kwargs['EOS'] = trg_loader.eos\n",
    "    eval_kwargs['PAD'] = trg_loader.pad\n",
    "    eval_kwargs['UNK'] = trg_loader.unk\n",
    "    logger = logging.getLogger(job_name)\n",
    "\n",
    "    # Make sure to be in evaluation mode\n",
    "    model = trainer.model\n",
    "    crit = trainer.criterion\n",
    "    model.eval()\n",
    "    src_loader.reset_iterator(split)\n",
    "    trg_loader.reset_iterator(split)\n",
    "    n = 0\n",
    "    loss_sum = 0\n",
    "    ml_loss_sum = 0\n",
    "    loss_evals = 0\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        batch_start = time.time()\n",
    "        # get batch\n",
    "        data_src, order = src_loader.get_src_batch(split, batch_size)\n",
    "        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n",
    "        n += batch_size\n",
    "        if model.version == 'seq2seq':\n",
    "            source = model.encoder(data_src)\n",
    "            source = model.map(source)\n",
    "            if trainer.criterion.version == \"seq\":\n",
    "                losses, stats = crit(model, source, data_trg)\n",
    "            else:  # ML & Token-level\n",
    "                # init and forward decoder combined\n",
    "                decoder_logit = model.decoder(source, data_trg)\n",
    "                losses, stats = crit(decoder_logit, data_trg['out_labels'])\n",
    "            batch_preds, _ = model.sample(source, eval_kwargs)\n",
    "        else:\n",
    "            losses, stats = crit(model(data_src, data_trg), data_trg['out_labels'])\n",
    "            batch_preds, _ = model.sample(data_src, eval_kwargs)\n",
    "\n",
    "        loss_sum += losses['final'].data.item()\n",
    "        ml_loss_sum += losses['ml'].data.item()\n",
    "        loss_evals = loss_evals + 1\n",
    "        # Initialize target with <BOS> for every sentence Index = 2\n",
    "        # print('batch preds', batch_preds)\n",
    "        if isinstance(batch_preds, list):\n",
    "            # wiht beam size unpadded preds\n",
    "            sent_preds = [decode_sequence(trg_loader.get_vocab(),\n",
    "                                          np.array(pred).reshape(1, -1),\n",
    "                                          eos=trg_loader.eos,\n",
    "                                          bos=trg_loader.bos)[0]\n",
    "                          for pred in batch_preds]\n",
    "        else:\n",
    "            # decode\n",
    "            sent_preds = decode_sequence(trg_loader.get_vocab(), batch_preds,\n",
    "                                         eos=trg_loader.eos,\n",
    "                                         bos=trg_loader.bos)\n",
    "        # Do the same for gold sentences\n",
    "        sent_source = decode_sequence(src_loader.get_vocab(),\n",
    "                                      data_src['labels'],\n",
    "                                      eos=src_loader.eos,\n",
    "                                      bos=src_loader.bos)\n",
    "        sent_gold = decode_sequence(trg_loader.get_vocab(),\n",
    "                                    data_trg['out_labels'],\n",
    "                                    eos=trg_loader.eos,\n",
    "                                    bos=trg_loader.bos)\n",
    "        if not verbose:\n",
    "            verb = not (n % 1000)\n",
    "        else:\n",
    "            verb = verbose\n",
    "        for (sl, l, gl) in zip(sent_source, sent_preds, sent_gold):\n",
    "            preds.append(l)\n",
    "            ground_truths.append(gl)\n",
    "            if verb:\n",
    "                lg.print_sampled(sl, gl, l)\n",
    "        lg.print_sampled(sent_source[0], sent_preds[0], sent_gold[0])\n",
    "        logger.info('Batch %d done in %.2f s', i, time.time() - batch_start)\n",
    "        if max_samples == -1:\n",
    "            ix1 = data_src['bounds']['it_max']\n",
    "        else:\n",
    "            ix1 = max_samples\n",
    "        if data_src['bounds']['wrapped']:\n",
    "            break\n",
    "        if n >= ix1:\n",
    "            break\n",
    "    logger.warn('Evaluated %d samples in %.2f s', len(preds), time.time()-start)\n",
    "    bleu_moses, _ = corpus_bleu(preds, ground_truths)\n",
    "    return preds, ml_loss_sum / loss_evals, loss_sum / loss_evals, bleu_moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt, pi, cos, ceil\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "# From https://github.com/elbayadm/attn2d/blob/master/nmt/optimizer.py\n",
    "TRACKERS = {\n",
    "    'train/loss': [],\n",
    "    'train/ml_loss': [],\n",
    "    'val/loss': [],\n",
    "    'val/ml_loss': [], \n",
    "    'val/perf/bleu': [],\n",
    "    'optim/lr': [], \n",
    "    'optim/grad_norm': [],\n",
    "    'optim/scheduled_sampling': [],\n",
    "    'optim/ntokens': [],\n",
    "    'optim/batch_size': [],\n",
    "    'iteration': 0,\n",
    "    'epoch': 1,\n",
    "    'batch_offset': 0,\n",
    "    'update': set(),\n",
    "    'devices': [],\n",
    "    'time': []\n",
    "}\n",
    "\n",
    "\n",
    "class InverseSquareRoot(lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Follow the schedule of Vaswani et al. 2017\n",
    "\n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/optimizer.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer,\n",
    "                 warmup=4000, last_epoch=-1):\n",
    "        self.warmup = warmup\n",
    "        super(InverseSquareRoot, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        it = self.last_epoch + 1\n",
    "        scale_factor = min(1 / sqrt(it), it / self.warmup ** 1.5)\n",
    "        return [base_lr * scale_factor\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ShiftedCosine(lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Similar to cosine\n",
    "\n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/optimizer.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, cycles, T_max, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.cycle_duration = ceil(T_max / cycles)\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        scale_factor = 1/2 * (cos(pi * (self.last_epoch % self.cycle_duration) / self.cycle_duration) + 1)\n",
    "        return [base_lr * scale_factor\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class PlateauCosine(lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Steep decrease then ~ plateau\n",
    "    [self.eta_min + (base_lr - self.eta_min) *\n",
    "                    (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                                    for base_lr in self.base_lrs]\n",
    "\n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/optimizer.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T1, T2, eta1, eta2, last_epoch=-1):\n",
    "        self.T1 = int(T1)\n",
    "        self.T2 = int(T2)\n",
    "        self.eta1 = float(eta1)\n",
    "        self.eta2 = float(eta2)\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.T1:\n",
    "            # use a wider period\n",
    "            return [self.eta2 + (self.eta1 - self.eta2) *\n",
    "                    (1 + cos(pi * self.last_epoch / self.T2)) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "        # very steep\n",
    "        return [self.eta1 + (base_lr - self.eta1) *\n",
    "                (1 + cos(pi * self.last_epoch / self.T1)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "def LRScheduler(opt, optimizer, last_epoch=-1):\n",
    "    \"\"\"\n",
    "    Learning rate scheduler.\n",
    "    \n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/optimizer.py\n",
    "    \"\"\"\n",
    "    ref = opt['schedule']\n",
    "    if ref == \"early-stopping\":\n",
    "        if opt['criterion'] == \"loss\":\n",
    "            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       patience=opt['patience'],\n",
    "                                                       factor=opt['decay_rate'],\n",
    "                                                       verbose=True,\n",
    "                                                       threshold=0.01,\n",
    "                                                       min_lr=1e-5)\n",
    "        elif opt['criterion'] == \"perf\":\n",
    "            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode=\"max\",\n",
    "                                                       patience=opt['patience'],\n",
    "                                                       factor=opt['decay_rate'],\n",
    "                                                       verbose=True,\n",
    "                                                       threshold=0.05)\n",
    "\n",
    "    elif ref == \"cosine-ep\":\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                   T_max=opt['max_epochs'],\n",
    "                                                   eta_min=opt.get('min_lr', 0))\n",
    "\n",
    "    elif ref == \"cosine\":\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                   T_max=opt['max_updates'],\n",
    "                                                   eta_min=opt.get('min_lr', 0))\n",
    "    elif ref == \"shifted-cosine\":\n",
    "        scheduler = ShiftedCosine(optimizer,\n",
    "                                  T_max=opt['max_updates'],\n",
    "                                  cycles=opt['cycles'])\n",
    "\n",
    "    elif ref == \"plateau-cosine\":\n",
    "        scheduler = PlateauCosine(optimizer,\n",
    "                                  T1=opt['T1'],\n",
    "                                  T2=opt['T2'],\n",
    "                                  eta1=opt['eta1'],\n",
    "                                  eta2=opt['eta2'])\n",
    "\n",
    "    elif ref == \"step\":\n",
    "        scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                        step_size=opt['decay_every'],\n",
    "                                        gamma=opt['decay_rate'],\n",
    "                                        last_epoch=last_epoch)\n",
    "        # self.lr_scheduler = lr_scheduler.LambdaLR(self.optimizer.optimizer, self.anneal)\n",
    "    elif ref == \"step-iter\":\n",
    "        scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                        step_size=opt['decay_every'],\n",
    "                                        gamma=opt['decay_rate'],\n",
    "                                        last_epoch=last_epoch)\n",
    "\n",
    "    elif ref == \"inverse-square\":\n",
    "        scheduler = InverseSquareRoot(optimizer,\n",
    "                                      warmup=opt['warmup'],\n",
    "                                      last_epoch=last_epoch)\n",
    "\n",
    "    elif ref == 'multi-step':\n",
    "        milestones = list(opt['milestones'].split(','))\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
    "                                             milestones,\n",
    "                                             gamma=opt['decay_rate'],\n",
    "                                             last_epoch=last_epoch)\n",
    "    else:\n",
    "        raise ValueError('Unknown scheduler % s' % ref)\n",
    "    scheduler.mode = ref\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Wrapper for the optimizer (fairseq style)\n",
    "    \n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/optimizer.py\n",
    "    \"\"\"\n",
    "    def __init__(self, opt, model):\n",
    "        super().__init__()\n",
    "        #  rmsprop | sgd | sgdmom | adagrad | adam\n",
    "        ref = opt['solver'].lower()\n",
    "        lr = opt['LR']['base']\n",
    "        if isinstance(model, list):\n",
    "            params = [{'params': m.parameters(),\n",
    "                       'lr': lr}\n",
    "                      for m in model]\n",
    "        else:\n",
    "            params = [{'params': model.parameters(), 'lr': lr}]\n",
    "\n",
    "        if ref == 'adam':\n",
    "            optimizer = optim.Adam(params,\n",
    "                                   lr=lr,\n",
    "                                   betas=(opt['alpha'], opt['beta']),\n",
    "                                   weight_decay=opt['weight_decay'],\n",
    "                                   eps=float(opt['epsilon']),\n",
    "                                   amsgrad=bool(opt.get('amsgrad', 0)))\n",
    "        elif ref == 'sgd':\n",
    "            optimizer = optim.SGD(params,\n",
    "                                  lr=lr,\n",
    "                                  momentum=opt.get('momentum', 0),\n",
    "                                  dampening=opt.get('dampening', 0),\n",
    "                                  weight_decay=opt['weight_decay'],\n",
    "                                  nesterov=bool(opt.get('nesterov', 0)))\n",
    "\n",
    "        elif ref.lower() == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(params,\n",
    "                                      lr=lr,\n",
    "                                      alpha=opt['alpha'],\n",
    "                                      eps=opt['epsilon'],\n",
    "                                      weight_decay=opt['weight_decay'],\n",
    "                                      momentum=opt.get('momentum', 0),\n",
    "                                      centered=False)\n",
    "        elif ref.lower() == 'adagrad':\n",
    "            optimizer = optim.Adagrad(params,\n",
    "                                      lr=lr,\n",
    "                                      lr_decay=opt.get('lr_decay', 0),\n",
    "                                      weight_decay=opt['weight_decay'],\n",
    "                                      initial_accumulator_value=0)\n",
    "        elif ref.lower() == 'nag':\n",
    "            optimizer = NAG(params,\n",
    "                            lr=lr,\n",
    "                            momentum=opt['momentum'],\n",
    "                            weight_decay=opt['weight_decay']\n",
    "                            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer % s' % ref)\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Return the current learning rate.\"\"\"\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        \"\"\"Set the learning rate.\"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Return the optimizer's state dict.\"\"\"\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load(self, state_dict):\n",
    "        \"\"\"Load an optimizer state dict. \"\"\"\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        return self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Clears the gradients of all optimized parameters.\"\"\"\n",
    "        return self.optimizer.zero_grad()\n",
    "\n",
    "    def require_grad(self):\n",
    "        \"\"\"Set requires_grad true for all params\"\"\"\n",
    "        for p in self.optimizer.param_groups:\n",
    "            if isinstance(p, dict):\n",
    "                for pp in p['params']:\n",
    "                    pp.requires_grad = True\n",
    "\n",
    "                    \n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Training a model with a given criterion.\n",
    "    \n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/trainer.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jobname, params, model, criterion):\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            raise NotImplementedError('Training on CPU is not supported')\n",
    "\n",
    "        self.params = params\n",
    "        self.jobname = jobname\n",
    "\n",
    "        self.logger = logging.getLogger(jobname)\n",
    "        # reproducibility:\n",
    "        set_seed(params['optim']['seed'])\n",
    "\n",
    "        self.clip_norm = params['optim']['grad_clip']\n",
    "        self.num_batches = params['optim']['num_batches']\n",
    "\n",
    "        # Move to GPU\n",
    "        self.model = model.cuda()\n",
    "        self.criterion = criterion.cuda()\n",
    "\n",
    "        # Initialize optimizer and LR scheduler\n",
    "        self.optimizer = Optimizer(params['optim'], model)\n",
    "        self.lr_patient = params['optim']['LR']['schedule'] == \"early-stopping\"\n",
    "        if self.lr_patient:\n",
    "            self.lr_patient = params['optim']['LR']['criterion']\n",
    "            self.logger.info('updating the lr wrt %s', self.lr_patient)\n",
    "        self.lr_scheduler = LRScheduler(params['optim']['LR'],\n",
    "                                        self.optimizer.optimizer,\n",
    "                                        )\n",
    "\n",
    "        self.tb_writer = SummaryWriter(params['eventname'])\n",
    "        self.log_every = params['track']['log_every']\n",
    "        self.checkpoint = params['track']['checkpoint']\n",
    "        self.evaluate = False\n",
    "        self.done = False\n",
    "        self.trackers = TRACKERS\n",
    "        self.iteration = 0\n",
    "        self.epoch = 0\n",
    "        self.batch_offset = 0\n",
    "        # Dump  the model params:\n",
    "        json.dump(params, open('%s/params.json' % params['modelname'], 'w'))\n",
    "\n",
    "    def update_params(self, val_loss=None):\n",
    "        \"\"\"\n",
    "        Update dynamic params:\n",
    "        lr, scheduled_sampling probability and tok/seq's alpha\n",
    "        \"\"\"\n",
    "        epoch = self.epoch\n",
    "        iteration = self.iteration\n",
    "        if not self.lr_patient:\n",
    "            if self.lr_scheduler.mode in [\"step-iter\", \"inverse-square\",\n",
    "                                          \"cosine\", 'shifted-cosine',\n",
    "                                          'plateau-cosine']:\n",
    "                self.lr_scheduler.step(iteration)\n",
    "            else:\n",
    "                self.lr_scheduler.step(epoch - 1)\n",
    "        self.track('optim/lr', self.optimizer.get_lr())\n",
    "\n",
    "    def step(self, data_src, data_trg, ntokens=0):\n",
    "        \"\"\"\n",
    "        A signle forward step\n",
    "        \"\"\"\n",
    "        # Clear the grads\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_size = data_src['labels'].size(0)\n",
    "        # evaluate the loss\n",
    "        decoder_logit = self.model(data_src, data_trg)\n",
    "        losses, stats = self.criterion(decoder_logit, data_trg['out_labels'])\n",
    "        if not ntokens:\n",
    "            ntokens = torch.sum(data_src['lengths'] *\n",
    "                                data_trg['lengths']).data.item()\n",
    "\n",
    "        return losses, batch_size, ntokens\n",
    "\n",
    "    def backward_step(self, loss, ml_loss, ntokens, nseqs, start, wrapped):\n",
    "        \"\"\"\n",
    "        A single backward step\n",
    "        \"\"\"\n",
    "        loss.backward()\n",
    "        if self.clip_norm > 0:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                                       self.clip_norm)\n",
    "        self.track('optim/grad_norm', grad_norm)\n",
    "        self.track('optim/ntokens', ntokens)\n",
    "        self.track('optim/batch_size', nseqs)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        # torch.cuda.empty_cache()  # FIXME\n",
    "        if np.isnan(loss.data.item()):\n",
    "            sys.exit('Loss is nan')\n",
    "        torch.cuda.synchronize()\n",
    "        self.iteration += 1\n",
    "        if wrapped:\n",
    "            self.epoch += 1\n",
    "        # Log\n",
    "        if (self.iteration % self.log_every == 0):\n",
    "            self.track('train/loss', loss.data.item())\n",
    "            self.track('train/ml_loss', ml_loss.data.item())\n",
    "            self.to_stderr(nseqs, ntokens, time.time()-start)\n",
    "            self.tensorboard()\n",
    "\n",
    "        self.evaluate = (self.iteration % self.checkpoint == 0)\n",
    "        self.done = (self.epoch > self.params['optim']['max_epochs'])\n",
    "\n",
    "    def validate(self, src_loader=None, trg_loader=None):\n",
    "        \"\"\"\n",
    "        Evaluate on the dev set\n",
    "        \"\"\"\n",
    "        params = self.params\n",
    "        self.log('Evaluating the model on the validation set..')\n",
    "        self.model.eval()\n",
    "        if params.get('eval_bleu', 1):\n",
    "            _, val_ml_loss, val_loss, bleu = evaluate_model(params['modelname'],\n",
    "                                                            self,\n",
    "                                                            src_loader,\n",
    "                                                            trg_loader,\n",
    "                                                            params['track'])\n",
    "            self.log('BLEU: %.5f ' % bleu)\n",
    "            self.track('val/perf/bleu', bleu)\n",
    "            self.log('logged bleu log')\n",
    "            save_best = (self.trackers['val/perf/bleu'][-1] ==\n",
    "                         max(self.trackers['val/perf/bleu']))\n",
    "            save_every = 0\n",
    "\n",
    "        else:\n",
    "            val_ml_loss, val_loss = evaluate_val_loss(params['modelname'],\n",
    "                                                      self,\n",
    "                                                      src_loader,\n",
    "                                                      trg_loader,\n",
    "                                                      params['track'])\n",
    "            save_every = 1\n",
    "            save_best = 0\n",
    "\n",
    "        self.track('val/loss', val_loss)\n",
    "        self.track('val/ml_loss', val_ml_loss)\n",
    "        self.tensorboard()\n",
    "        # Save model if still improving on the dev set\n",
    "        # self.save_model(src_loader, trg_loader, save_best, save_every)\n",
    "        # self.model.train()\n",
    "        if self.lr_patient == \"loss\":\n",
    "            self.log('Updating the learning rate - LOSS')\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "            self.track('optim/lr', self.optimizer.get_lr())\n",
    "        elif self.lr_patient == \"perf\":\n",
    "            assert not save_every\n",
    "            self.log('Updating the learning rate - PERF')\n",
    "            self.lr_scheduler.step(bleu)\n",
    "            self.track('optim/lr', self.optimizer.get_lr())\n",
    "\n",
    "    def save_model(self, src_loader, trg_loader, save_best, save_every):\n",
    "        \"\"\"\n",
    "        checkoint model, optimizer and history\n",
    "        \"\"\"\n",
    "        params = self.params\n",
    "        modelname = params['modelname']\n",
    "        checkpoint_path = osp.join(modelname, 'model.pth')\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        self.log(\"model saved to {}\".format(checkpoint_path))\n",
    "        optimizer_path = osp.join(modelname, 'optimizer.pth')\n",
    "        torch.save(self.optimizer.state_dict(), optimizer_path)\n",
    "        self.log(\"optimizer saved to {}\".format(optimizer_path))\n",
    "        self.trackers['src_iterators'] = src_loader.iterators\n",
    "        self.trackers['trg_iterators'] = trg_loader.iterators\n",
    "        self.trackers['iteration'] = self.iteration\n",
    "        self.trackers['epoch'] = self.epoch\n",
    "        pdump(self.trackers, osp.join(modelname, 'trackers.pkl'))\n",
    "\n",
    "        if save_best:\n",
    "            checkpoint_path = osp.join(modelname, 'model-best.pth')\n",
    "            torch.save(self.model.state_dict(), checkpoint_path)\n",
    "            self.log(\"model saved to {}\".format(checkpoint_path))\n",
    "            optimizer_path = osp.join(modelname, 'optimizer-best.pth')\n",
    "            torch.save(self.optimizer.state_dict(), optimizer_path)\n",
    "            self.log(\"optimizer saved to {}\".format(optimizer_path))\n",
    "            pdump(self.trackers, osp.join(modelname, 'trackers-best.pkl'))\n",
    "\n",
    "        if save_every:\n",
    "            checkpoint_path = osp.join(modelname, 'model-%d.pth' % self.iteration)\n",
    "            torch.save(self.model.state_dict(), checkpoint_path)\n",
    "           self.log(\"model saved to {}\".format(checkpoint_path))\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Load last saved params:\n",
    "        for use with oar's idempotent jobs\n",
    "        \"\"\"\n",
    "        params = self.params\n",
    "        modelname = params['modelname']\n",
    "        iterators_state = {}\n",
    "        history = {}\n",
    "        if osp.exists(osp.join(modelname, 'model.pth')):\n",
    "            self.warn('Picking up where we left')\n",
    "            # load model's weights\n",
    "            saved_state = torch.load(osp.join(modelname, 'model.pth'))\n",
    "            saved = list(saved_state)\n",
    "            required_state = self.model.state_dict()\n",
    "            required = list(required_state)\n",
    "            del required_state\n",
    "            if \"module\" in required[0] and \"module\" not in saved[0]:\n",
    "                for k in saved:\n",
    "                    kbis = \"module.%s\" % k\n",
    "                    saved_state[kbis] = saved_state[k]\n",
    "                    del saved_state[k]\n",
    "\n",
    "            for k in saved:\n",
    "                if \"increment\" in k:\n",
    "                    del saved_state[k]\n",
    "                if \"transiton\" in k:\n",
    "                    kk = k.replace(\"transiton\", \"transition\")\n",
    "                    saved_state[kk] = saved_state[k]\n",
    "                    del saved_state[k]\n",
    "            self.model.load_state_dict(saved_state)\n",
    "            # load the optimizer's last state:\n",
    "            self.optimizer.load(\n",
    "                torch.load(osp.join(modelname, 'optimizer.pth')\n",
    "                           ))\n",
    "            history = pload(osp.join(modelname, 'trackers.pkl'))\n",
    "            iterators_state = {'src_iterators': history['src_iterators'],\n",
    "                               'trg_iterators': history['trg_iterators']}\n",
    "\n",
    "        elif params['start_from']:\n",
    "            start_from = params['start_from']\n",
    "            # Start from a pre-trained model:\n",
    "            self.warn('Starting from %s' % start_from)\n",
    "            if params['start_from_best']:\n",
    "                flag = '-best'\n",
    "                self.warn('Starting from the best saved model')\n",
    "            else:\n",
    "                flag = ''\n",
    "            # load model's weights\n",
    "            self.model.load_state_dict(\n",
    "                    torch.load(osp.join(start_from, 'model%s.pth' % flag))\n",
    "                    )\n",
    "            # load the optimizer's last state:\n",
    "            if not params['optim']['reset']:\n",
    "                self.optimizer.load(\n",
    "                    torch.load(osp.join(start_from, 'optimizer%s.pth' % flag)\n",
    "                               ))\n",
    "            history = pload(osp.join(start_from, 'trackers%s.pkl' % flag))\n",
    "        self.trackers.update(history)\n",
    "        self.epoch = self.trackers['epoch']\n",
    "        self.iteration = self.trackers['iteration']\n",
    "        return iterators_state\n",
    "\n",
    "    def log(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def warn(self, message):\n",
    "        self.logger.warning(message)\n",
    "\n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "\n",
    "    def set_devices(self, devices):\n",
    "        self.trackers['devices'].append(devices)\n",
    "        self.trackers['time'].append(0)\n",
    "\n",
    "    def increment_time(self, t):\n",
    "        self.trackers['time'][-1] += t\n",
    "\n",
    "    def track(self, k, v):\n",
    "        \"\"\"\n",
    "        Track key metrics\n",
    "        \"\"\"\n",
    "        if k not in self.trackers:\n",
    "            raise ValueError('Tracking unknown entity %s' % k)\n",
    "        if isinstance(self.trackers[k], list):\n",
    "            self.trackers[k].append(v)\n",
    "        else:\n",
    "            self.trackers[k] = v\n",
    "        self.trackers['update'].add(k)\n",
    "\n",
    "    def tensorboard(self):\n",
    "        \"\"\"\n",
    "        Write tensorboard events\n",
    "        \"\"\"\n",
    "        for k in self.trackers['update']:\n",
    "            self.tb_writer.add_scalar(k, self.trackers[k][-1], self.iteration)\n",
    "        self.tb_writer.file_writer.flush()\n",
    "        self.trackers['update'] = set()\n",
    "\n",
    "    def to_stderr(self, batch_size, ntokens, timing):\n",
    "        \"\"\"\n",
    "        Log to stderr\n",
    "        \"\"\"\n",
    "        self.log('| epoch {:2d} '\n",
    "                 '| iteration {:5d} '\n",
    "                 '| lr {:02.2e} '\n",
    "                 '| seq {:3d} '\n",
    "                 '| sXt {:5d} '\n",
    "                 '| ms/batch {:6.3f} '\n",
    "                 '| total time {:6.2f} s'\n",
    "                 '| loss {:6.3f} '\n",
    "                 '| ml {:6.3f}'\n",
    "                 .format(self.epoch,\n",
    "                         self.iteration,\n",
    "                         self.optimizer.get_lr(),\n",
    "                         batch_size,\n",
    "                         ntokens,\n",
    "                         timing * 1000,\n",
    "                         sum(self.trackers['time']),\n",
    "                         self.trackers['train/loss'][-1],\n",
    "                         self.trackers['train/ml_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLCriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    The default cross entropy loss.\n",
    "    \n",
    "    From https://github.com/elbayadm/attn2d/blob/master/nmt/loss/cross_entropy.py\n",
    "    \"\"\"\n",
    "    def __init__(self, job_name, params):\n",
    "        super().__init__()\n",
    "        self.logger = logging.getLogger(job_name)\n",
    "        self.th_mask = params.get('mask_threshold', 1)  # both pad and unk\n",
    "        self.normalize = params.get('normalize', 'ntokens')\n",
    "        self.version = 'ml'\n",
    "\n",
    "    def log(self):\n",
    "        self.logger.info('Default ML loss')\n",
    "\n",
    "    def forward(self, logp, target):\n",
    "        \"\"\"\n",
    "        logp : the decoder logits (N, seq_length, V)\n",
    "        target : the ground truth labels (N, seq_length)\n",
    "        \"\"\"\n",
    "        output = self.get_ml_loss(logp, target)\n",
    "        return {\"final\": output, \"ml\": output}, {}\n",
    "\n",
    "    def get_ml_loss(self, logp, target):\n",
    "        \"\"\"\n",
    "        Compute the usual ML loss.\n",
    "        \"\"\"\n",
    "        # print('logp:', logp.size(), \"target:\", target.size())\n",
    "        batch_size = logp.size(0)\n",
    "        seq_length = logp.size(1)\n",
    "        vocab = logp.size(2)\n",
    "        target = target[:, :seq_length]\n",
    "        logp = to_contiguous(logp).view(-1, logp.size(2))\n",
    "        target = to_contiguous(target).view(-1, 1)\n",
    "        mask = target.gt(self.th_mask)\n",
    "        ml_output = - logp.gather(1, target)[mask]\n",
    "        ml_output = torch.sum(ml_output)\n",
    "\n",
    "        if self.normalize == 'ntokens':\n",
    "            # print('initial ml:', ml_output.data.item())\n",
    "            norm = torch.sum(mask)\n",
    "            ml_output /= norm.float()\n",
    "            # print('norm ml:', ml_output.data.item(), '// %d' % norm.data.item())\n",
    "        elif self.normalize == 'seqlen':\n",
    "            # print('initial ml:', ml_output.data.item())\n",
    "            norm = seq_length\n",
    "            ml_output /= norm\n",
    "            # print('norm ml:', ml_output.data.item(), '// %d' % norm)\n",
    "        elif self.normalize == 'batch':\n",
    "            # print('initial ml:', ml_output.data.item())\n",
    "            norm = batch_size\n",
    "            ml_output /= norm\n",
    "            # print('norm ml:', ml_output.data.item(), '// %d' % norm)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unknown normalizing scheme')\n",
    "        return ml_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code.\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class VocabData(object):\n",
    "    def __init__(self, infos_fn):\n",
    "        # Load index to word mapping from .infos file.\n",
    "        self.infos_filename = infos_fn\n",
    "        infos = pload(self.infos_filename)\n",
    "        self.ix_to_word = infos['itow']\n",
    "        self.vocab_size = len(self.ix_to_word)\n",
    "\n",
    "        # Word to index mapping and special tokens.\n",
    "        word_to_ix = {w: ix for ix, w in self.ix_to_word.items()}\n",
    "        self.pad = word_to_ix['<PAD>']\n",
    "        self.unk = word_to_ix['<UNK>']\n",
    "        self.eos = word_to_ix['<EOS>']\n",
    "        self.bos = word_to_ix['<BOS>']\n",
    "\n",
    "\n",
    "class TextDataLoader(object):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/elbayadm/attn2d/blob/master/nmt/loader/dataloader.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_infos, src_h5, tgt_infos, tgt_h5, batch_size,\n",
    "                 max_length, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.logger = logging.getLogger(model_name)\n",
    "\n",
    "        self.src_vocab = VocabData(src_infos)\n",
    "        self.tgt_vocab = VocabData(tgt_infos)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = max_length\n",
    "        self.logger.info(f'Loading h5 files: {src_h5}, {tgt_h5}')\n",
    "\n",
    "        # Load HDF5 data file.\n",
    "        self.datasets = {}\n",
    "        self.loaders = {}\n",
    "        self.max_indices = { 'train': 0, 'val': 0, 'test': 0 }\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            self.datasets[split] = ZippedDataset(\n",
    "                'src': TextDataset(src_h5, split),\n",
    "                'tgt': TextDataset(tgt_h5, split))\n",
    "            sampler = \\\n",
    "                torch.utils.data.distributed.DistributedSampler(self.dataset[split])\n",
    "            self.loaders[split] = torch.utils.data.DataLoader(\n",
    "                self.datasets[split], batch_size=self.batch_size,\n",
    "                shuffle=False, num_workers=1, pin_memory=True, sampler=sampler)\n",
    "        self.logger.info(\n",
    "            'Train:  {} | Dev: {} | Test: {}'.format(\n",
    "                len(self.datasets['train']),\n",
    "                len(self.datasets['val']),\n",
    "                len(self.datasets['test'])))\n",
    "        self.logger.warning(f'Reading sequences up to {self.seq_length}')\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.ix_to_word\n",
    "\n",
    "    def get_seq_length(self):\n",
    "        return self.seq_length\n",
    "\n",
    "\n",
    "class H5Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Text data iterator class\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_filename, dsname):\n",
    "        super().__init__()\n",
    "        self.data_info = {}\n",
    "\n",
    "        self.h5_filename = h5_filename\n",
    "        self.dsname = dsname\n",
    "        with h5py.File(self.h5_filename, 'r', libver='latest', swmr=True) as h5_file:\n",
    "            self.data_cache = np.array(h5_file[dsname])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_cache[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_cache)\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, h5_filename, split):\n",
    "        super().__init__()\n",
    "        self.h5_filename = h5_filename\n",
    "\n",
    "        self.label_ds = H5Dataset(self.h5_filename, f'labels_{split}')\n",
    "        self.length_ds = H5Dataset(self.ds_filename, f'lengths_{split}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.length_ds)\n",
    "\n",
    "\n",
    "class ZippedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset1, dataset2):\n",
    "        super().__init__()\n",
    "        assert(len(dataset1) == len(dataset2))\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.dataset1[index], self.dataset2[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from nmt.params import parse_params, set_env\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_worker(pindex, gpu_ids, params):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/elbayadm/attn2d/blob/master/train.py\n",
    "    \"\"\"\n",
    "    model_name = params['model_name']\n",
    "    logger = PervasiveLogger(model_name)\n",
    "\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '3892'\n",
    "    if not os.getenv('RANK', None):\n",
    "        os.environ['RANK'] = str(pindex)\n",
    "\n",
    "    device_id = gpu_ids[pindex]\n",
    "    device_name = torch.cuda.get_device_name(device_id)\n",
    "    torch.cuda.set_device(device_id)\n",
    "    logger.info(f'Device {pindex}: cuda({device_id}), {device_name}')\n",
    "\n",
    "    project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    comm_file = f'{project_dir}/{model_name}/pgroup_shared'\n",
    "    if os.path.isfile(comm_file):\n",
    "        os.remove(comm_file)\n",
    "    logger.info(f'Process communication file: {comm_file}')\n",
    "    torch.distributed.init_process_group(\n",
    "            backend='nccl',\n",
    "            world_size=len(gpu_ids),\n",
    "            rank=pindex,\n",
    "            init_method=f'file://{comm_file}')\n",
    "\n",
    "    if 'network' in params and 'type' in params['network']:\n",
    "        net_type = params['network']['type']\n",
    "    else:\n",
    "        net_type = 'efficient-densenet'\n",
    "    if 'encoder' in params:\n",
    "        enc_in_dim = (\n",
    "            params['encoder']['input'] if 'input' in params['encoder'] else 128)\n",
    "        enc_dropout = (\n",
    "            params['encoder']['dropout'] if 'dropout' in params['encoder'] else 0.2)\n",
    "    if 'decoder' in params:\n",
    "        dec_in_dim = (\n",
    "            params['decoder']['input'] if 'input' in params['decoder'] else 128)\n",
    "        dec_dropout = (\n",
    "            params['decoder']['dropout'] if 'dropout' in params['decoder'] else 0.2)\n",
    "        pred_dropout = (\n",
    "            params['decoder']['prediction_dropout'] if 'prediction_dropout' in params['decoder'] else 0.2)\n",
    "    model = Pervasive(model_name, net_type, Ts, Tt, special_tokens,\n",
    "                      enc_in_dim, enc_dropout, dec_in_dim, dec_dropout,\n",
    "                      pred_dropout)\n",
    "    model.init_weights()\n",
    "    model.cuda(device_id)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[device_id])\n",
    "    batch_size = params['data']['batch_size'] / len(gpu_ids)\n",
    "\n",
    "    criterion = loss.MLCriterion(model_name, params)\n",
    "    criterion.log()\n",
    "    trainer = Trainer(model_name, params, model, criterion)\n",
    "\n",
    "    src_infos = os.path.join(params['data']['dir'], f'{params[\"src\"]}.infos')\n",
    "    tgt_infos = os.path.join(params['data']['dir'], f'{params[\"tgt\"]}.infos')\n",
    "    src_h5 = os.path.join(params['data']['dir'], f'{params[\"src\"]}.h5')\n",
    "    tgt_h5 = os.path.join(params['data']['dir'], f'{params[\"tgt\"]}.h5')\n",
    "    train_loader = TextDataLoader(src_infos, src_h5, tgt_infos, tgt_h5, batch_size,\n",
    "                                  params['data']['max_length'], model_name):\n",
    "\n",
    "    iters = trainer.load_checkpoint()\n",
    "    if trainer.lr_patient:\n",
    "        trainer.update_params()\n",
    "\n",
    "    for epoch in range(params['optim']['max_epochs']):\n",
    "        # Update learning rate.\n",
    "        if not trainer.lr_patient:\n",
    "            trainer.update_params()\n",
    "        torch.cuda.synchronize()\n",
    "        avg_loss = torch.zeros(1).cuda()\n",
    "        avg_ml_loss = torch.zeros(1).cuda()\n",
    "        total_ntokens = 0\n",
    "        total_nseqs = 0\n",
    "        start = time.time()\n",
    "        for src_data, tgt_data in loader.loaders['train']:\n",
    "            losses, batch_size, ntokens = trainer.step(src_data, tgt_data)\n",
    "            avg_loss += ntokens * losses['final']\n",
    "            avg_ml_loss += ntokens * losses['ml']\n",
    "            total_nseqs += batch_size\n",
    "            total_ntokens += ntokens\n",
    "            avg_loss /= total_ntokens\n",
    "            avg_ml_loss /= total_ntokens\n",
    "\n",
    "        trainer.backward_step(avg_loss, avg_ml_loss,\n",
    "                              total_ntokens, total_nseqs,\n",
    "                              start, False)\n",
    "        trainer.increment_time(time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
